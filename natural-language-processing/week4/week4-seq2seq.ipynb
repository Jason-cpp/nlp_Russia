{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to calculate with seq2seq model\n",
    "\n",
    "In this assignment, you will learn how to use neural networks to solve sequence-to-sequence prediction tasks. Seq2Seq models are very popular these days because they achieve great results in Machine Translation, Text Summarization, Conversational Modeling and more.\n",
    "\n",
    "Using sequence-to-sequence modeling you are going to build a calculator for evaluating arithmetic expressions, by taking an equation as an input to the neural network and producing an answer as it's output.\n",
    "\n",
    "The resulting solution for this problem will be based on state-of-the-art approaches for sequence-to-sequence learning and you should be able to easily adapt it to solve other tasks. However, if you want to train your own machine translation system or intellectual chat bot, it would be useful to have access to compute resources like GPU, and be patient, because training of such systems is usually time consuming. \n",
    "\n",
    "### Libraries\n",
    "\n",
    "For this task you will need the following libraries:\n",
    " - [TensorFlow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
    " - [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    " \n",
    "If you have never worked with TensorFlow, you will probably want to read some tutorials during your work on this assignment, e.g. [Neural Machine Translation](https://www.tensorflow.org/tutorials/seq2seq) tutorial deals with very similar task and can explain some concepts to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    使用神经网络解决sequence-to-sequence 任务\\n    sequence-to-sequence 模型可以用于解决很多问题，例如机器翻译，文本总结，对话模型等,并且取得非常好的效果\\n    在这个任务中，将会使用seq2seq模型来为数学表达式建立一个计算器，将一个计算式作为神经网络的输入，计算出结果作为神经网络的输出\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    使用神经网络解决sequence-to-sequence 任务\n",
    "    sequence-to-sequence 模型可以用于解决很多问题\n",
    "        例如机器翻译，文本总结，对话模型等,并且取得非常好的效果\n",
    "    在这个任务中，将会使用seq2seq模型来为数学表达式建立一个计算器，将一个计算式作为神经网络的输入，计算出结果作为神经网络的输出\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "One benefit of this task is that you don't need to download any data — you will generate it on your own! We will use two operators (addition and subtraction) and work with positive integer numbers in some range. Here are examples of correct inputs and outputs:\n",
    "\n",
    "    Input: '1+2'\n",
    "    Output: '3'\n",
    "    \n",
    "    Input: '0-99'\n",
    "    Output: '-99'\n",
    "\n",
    "*Note, that there are no spaces between operators and operands.*\n",
    "\n",
    "\n",
    "Now you need to implement the function *generate_equations*, which will be used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
    "    \"\"\"Generates pairs of equations and solutions to them.\n",
    "    \n",
    "       Each equation has a form of two integers with an operator in between.\n",
    "       Each solution is an integer with the result of the operaion.\n",
    "    \n",
    "        allowed_operators: list of strings, allowed operators.\n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "\n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    for _ in range(dataset_size):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        value1 = random.randint(min_value, max_value)\n",
    "        value2 = random.randint(min_value, max_value)\n",
    "        index = random.randint(0,len(allowed_operators)-1)\n",
    "        if allowed_operators[index] == \"+\":\n",
    "            solution = value1 + value2\n",
    "        else:\n",
    "            solution = value1 - value2\n",
    "        equation = str(value1) + allowed_operators[index] + str(value2)\n",
    "        tup =(equation, str(solution))\n",
    "        sample.append(tup)\n",
    "    #print(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the correctness of your implementation, use *test_generate_equations* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generate_equations():\n",
    "    allowed_operators = ['+', '-']\n",
    "    dataset_size = 10\n",
    "    for (input_, output_) in generate_equations(allowed_operators, dataset_size, 0, 100):\n",
    "        if not (type(input_) is str and type(output_) is str):\n",
    "            return \"Both parts should be strings.\"\n",
    "        if eval(input_) != int(output_):\n",
    "            return \"The (equation: {!r}, solution: {!r}) pair is incorrect.\".format(input_, output_)\n",
    "    return \"Tests passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_generate_equations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to generate the train and test data for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#生成了十万个训练样本\n",
    "allowed_operators = ['+', '-']\n",
    "dataset_size = 100000\n",
    "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=9999)\n",
    "#数据集切分\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the neural network\n",
    "\n",
    "The next stage of data preparation is creating mappings of the characters to their indices in some vocabulary. Since in our task we already know which symbols will appear in the inputs and outputs, generating the vocabulary is a simple step.\n",
    "\n",
    "#### How to create dictionaries for other task\n",
    "\n",
    "First of all, you need to understand what is the basic unit of the sequence in your task. In our case, we operate on symbols and the basic unit is a symbol. The number of symbols is small, so we don't need to think about filtering/normalization steps. However, in other tasks, the basic unit is often a word, and in this case the mapping would be *word $\\to$ integer*. The number of words might be huge, so it would be reasonable to filter them, for example, by frequency and leave only the frequent ones. Other strategies that your should consider are: data normalization (lowercasing, tokenization, how to consider punctuation marks), separate vocabulary for input and for output (e.g. for machine translation), some specifics of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    创建一个从字符到下标的映射，在我们的任务中已经知道了哪些符号会出现在输入输出中\\n    首先需要理解在我的任务中，序列的基础单元的是什么．在这个案例中，我们处理的基础单元是字符．这个字符的总数量很少，所以不需要进行过滤．\\n    对于其他的任务，理解序列的基础单元，一般来说是词，那这种映射就是 word-->integer．\\n    词汇量非常巨大，所以一般会过滤掉一些，仅仅留下高频词，还要用到数据的规范化（大小写，符号标记等），为输入和输出准备不同的词汇表等\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    创建一个从字符到下标的映射，在我们的任务中已经知道了哪些符号会出现在输入输出中\n",
    "    首先需要理解在我的任务中，序列的基础单元的是什么．在这个案例中，我们处理的基础单元是字符．这个字符的总数量很少，所以不需要进行过滤．\n",
    "    对于其他的任务，理解序列的基础单元，一般来说是词，那这种映射就是 word-->integer．\n",
    "    词汇量非常巨大，所以一般会过滤掉一些，仅仅留下高频词，还要用到数据的规范化（大小写，符号标记等），为输入和输出准备不同的词汇表等\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#enumerate：枚举变量，可以形成 index,character,形成了一个字典\n",
    "word2id = {symbol:i for i, symbol in enumerate('^$#+-1234567890')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '^', 1: '$', 2: '#', 3: '+', 4: '-', 5: '1', 6: '2', 7: '3', 8: '4', 9: '5', 10: '6', 11: '7', 12: '8', 13: '9', 14: '0'}\n"
     ]
    }
   ],
   "source": [
    "#print(word2id)\n",
    "#print(len(word2id))\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_symbol = '^'       #表示解码过程的开始\n",
    "end_symbol = '$'         #用于字符串的结尾，包括输入和输出序列\n",
    "padding_symbol = '#'     #填充字符串至同一长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could notice that we have added 3 special symbols: '^', '\\$' and '#':\n",
    "- '^' symbol will be passed to the network to indicate the beginning of the decoding procedure. We will discuss this one later in more details.\n",
    "- '\\$' symbol will be used to indicate the *end of a string*, both for input and output sequences. \n",
    "- '#' symbol will be used as a *padding* character to make lengths of all strings equal within one training batch.\n",
    "\n",
    "People have a bit different habits when it comes to special symbols in encoder-decoder networks, so don't get too much confused if you come across other variants in tutorials you read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When vocabularies are ready, we need to be able to convert a sentence to a list of vocabulary word indices and back. At the same time, let's care about padding. We are going to preprocess each sequence from the input (and output ground truth) in such a way that:\n",
    "- it has a predefined length *padded_len*\n",
    "- it is probably cut off or padded with the *padding symbol* '#'\n",
    "- it *always* ends with the *end symbol* '$'\n",
    "\n",
    "We will treat the original characters of the sequence **and the end symbol** as the valid part of the input. We will store *the actual length* of the sequence, which includes the end symbol, but does not include the padding symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    将原始序列的字符以及最后的一个结束标志都作为输入的有效部分，这也是要存储的序列实际长度．包括结尾字符，但是不包括填充符号\\n    把每个句子转换成一个有词汇下标组成的列表，反之，也必须可以做到\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    将原始序列的字符以及最后的一个结束标志都作为输入的有效部分，这也是要存储的序列实际长度．包括结尾字符，但是不包括填充符号\n",
    "    把每个句子转换成一个有词汇下标组成的列表，反之，也必须可以做到\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you need to implement the function *sentence_to_ids* that does the described job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    将句子转换为下标列表，超过了规定长度就剪掉后续序列，不够就补充至规定长度\n",
    "\"\"\"\n",
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    #print(\"type(sentence):\", type(sentence))\n",
    "#     for i in sentence:\n",
    "#         if i not in word2id.keys():\n",
    "#             #print(\"跑哪去了：\", i)\n",
    "\n",
    "    #-1都是因为要考虑到一个结尾字符\"$\"\n",
    "    sent_ids = [word2id[i] for i in sentence] ######### YOUR CODE HERE #############\n",
    "    if len(sent_ids) >= (padded_len-1):\n",
    "        sent_ids = sent_ids[0:padded_len-1]\n",
    "    sent_ids.append(1)\n",
    "    sent_len = len(sent_ids)                  ######### YOUR CODE HERE #############\n",
    "    while(len(sent_ids) < (padded_len) ):\n",
    "        sent_ids.append(2)\n",
    "    \n",
    "    return sent_ids, sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_sentence_to_ids():\n",
    "    sentences = [(\"123+123\", 7), (\"123+123\", 8), (\"123+123\", 10)]\n",
    "    expected_output = [([5, 6, 7, 3, 5, 6, 1], 7), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1], 8), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 1, 2, 2], 8)] \n",
    "    for (sentence, padded_len), (sentence_ids, expected_length) in zip(sentences, expected_output):\n",
    "        #print(\"sentence:\", sentence)\n",
    "        output, length = sentence_to_ids(sentence, word2id, padded_len)\n",
    "        print(output)\n",
    "        if output != sentence_ids:\n",
    "            return(\"Convertion of '{}' for padded_len={} to {} is incorrect.\".format(\n",
    "                sentence, padded_len, output))\n",
    "        if length != expected_length:\n",
    "            return(\"Convertion of '{}' for padded_len={} has incorrect actual length {}.\".format(\n",
    "                sentence, padded_len, length))\n",
    "    return(\"Tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 3, 5, 6, 1]\n",
      "[5, 6, 7, 3, 5, 6, 7, 1]\n",
      "[5, 6, 7, 3, 5, 6, 7, 1, 2, 2]\n",
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_to_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to get back from indices to symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    把下标转换回句子\n",
    "\"\"\"\n",
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of data preparation is a function that transforms a batch of sentences to a list of lists of indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentences：传进来的tuple的列表\n",
    "#官方代码：一次确实只能处理一个句子\n",
    "#如果batch_to_ids一次可以处理一个句子集合，我的问题就迎刃而解了\n",
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    #最大长度:最长句子的长度和max_len中选择一个更小的\n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)  #+1是指结尾字符\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *generate_batches* will help to generate batches with defined size from given samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用来批量的产生样本\n",
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the result of the implemented functions, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('8579-7887', '692')\n",
      "type(sentences): <class 'tuple'>\n",
      "Ids: [[12, 9, 11, 13, 4, 11, 12, 12, 11, 1], [10, 13, 6, 1, 2, 2, 2, 2, 2, 2]]\n",
      "Sentences lengths: [10, 4]\n"
     ]
    }
   ],
   "source": [
    "# sentences = []\n",
    "# sentences.append(train_set[0])\n",
    "# sentences.append(train_set[1])\n",
    "sentences = train_set[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)    #X和Y的最大长度都是10\n",
    "print('Input:', sentences)\n",
    "print(\"type(sentences):\", type(sentences))\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))    #返回的是有效字符串的长度，包括结尾字符-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder architecture\n",
    "\n",
    "Encoder-Decoder is a successful architecture for Seq2Seq tasks with different lengths of input and output sequences. The main idea is to use two recurrent neural networks, where the first neural network *encodes* the input sequence into a real-valued vector and then the second neural network *decodes* this vector into the output sequence. While building the neural network, we will specify some particular characteristics of this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Encoder-Decoder\\u3000是一个用来解决Seq2Seq任务的成功的架构，输入和输出的序列不一样．主要的思想是使用两种递归神经网络，\\n    第一个网络将输入序列变成一个实值向量，第二个神经网络将这个实值向量解码成输出序列\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Encoder-Decoder　是一个用来解决Seq2Seq任务的成功的架构，输入和输出的序列不一样．主要的思想是使用两种递归神经网络，\n",
    "    第一个网络将输入序列变成一个实值向量，第二个神经网络将这个实值向量解码成输出序列\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use TensorFlow building blocks to specify the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create [placeholders](https://www.tensorflow.org/api_guides/python/io_ops#Placeholders) to specify what data we are going to feed into the network during the execution time. For this task we will need:\n",
    " - *input_batch* — sequences of sentences (the shape will equal to [batch_size, max_sequence_len_in_batch]);\n",
    " - *input_batch_lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *ground_truth* — sequences of groundtruth (the shape will equal to [batch_size, max_sequence_len_in_batch]);\n",
    " - *ground_truth_lengths* — lengths of not padded groundtruth sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predifined value 1;\n",
    " - *learning_rate_ph* — learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_batch：输入句子Ｘ的序列\\ninput_batch_lengths：没有填充的句子的长度，即Ｘ的长度\\nground_truth：输入Ｘ对应的结果Ｙ的序列\\nground_truth_lengths：Y的长度\\n留存率\\n学习率\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_batch：输入句子Ｘ的序列\n",
    "input_batch_lengths：没有填充的句子的长度，即Ｘ的长度\n",
    "ground_truth：输入Ｘ对应的结果Ｙ的序列\n",
    "ground_truth_lengths：Y的长度\n",
    "留存率\n",
    "学习率\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "    \n",
    "    # Placeholders for input and its actual lengths.\n",
    "    # shape, dtype, name\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')    #batch_size * sequence_length\n",
    "    #print(\"type(self.input_batch):\", type(self.input_batch))\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')    #batch_size\n",
    "    \n",
    "    # Placeholders for groundtruth and its actual lengths.\n",
    "    self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\"ground_truth\")    #batch_size * sequence_length\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name=\"ground_truth_lengths\")    #batch_size\n",
    "        \n",
    "    #tf.cast(x, dtype, name=None) 将ｘ的数据格式转化成dtype\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])######### YOUR CODE HERE ############# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to prepare an embedding matrix. Since we use the same vocabulary for input and output, we need only one such matrix. For tasks with different vocabularies there would be multiple embedding layers.\n",
    "- Create embeddings matrix with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable). Specify its name, type (tf.float32), and initialize with random values.\n",
    "- Perform [embeddings lookup](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) for a given input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeddings(self, vocab_size, embeddings_size):\n",
    "    \"\"\"Specifies embeddings layer and embeds an input batch.\"\"\"\n",
    "     \n",
    "    #初始化 vocab_size:15    embeddings_size:512?\n",
    "    random_initializer = tf.random_uniform((vocab_size, embeddings_size), -1.0, 1.0)\n",
    "    self.embeddings = tf.Variable(initial_value=random_initializer, dtype=tf.float32, name=\"embeddings\")######### YOUR CODE HERE ############# \n",
    "    \n",
    "    # Perform embeddings lookup for self.input_batch. \n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)######### YOUR CODE HERE ############# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__create_embeddings = classmethod(create_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The first RNN of the current architecture is called an *encoder* and serves for encoding an input sequence to a real-valued vector. Input of this RNN is an embedded input batch. Since sentences in the same batch could have different actual lengths, we also provide input lengths to avoid unnecessary computations. The final encoder state will be passed to the second RNN (decoder), which we will create soon. \n",
    "\n",
    "- TensorFlow provides a number of [RNN cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods) ready for use. We suggest that you use [GRU cell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GRUCell), but you can also experiment with other types. \n",
    "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify input keep probability using the dropout placeholder that we created before.\n",
    "- Combine the defined encoder cells with [Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). Use the embedded input batches and their lengths here.\n",
    "- Use *dtype=tf.float32* everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    这个架构中的第一个RNN网络是编码器，用于将输入的序列编码成一个实值向量．这个RNN的输入是一个输入的embedded矩阵．\\n    在相同batch中的句子也可以有不同的实际长度，会提供输出长度来避免不必要的计算．编码器的最后一个状态会通过第二个RNN网络．\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    这个架构中的第一个RNN网络是编码器，用于将输入的序列编码成一个实值向量．这个RNN的输入是一个输入的embedded矩阵．\n",
    "    在相同batch中的句子也可以有不同的实际长度，会提供输出长度来避免不必要的计算．编码器的最后一个状态会通过第二个RNN网络．\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_encoder(self, hidden_size):\n",
    "    \"\"\"Specifies encoder architecture and computes its output.\"\"\"\n",
    "    \n",
    "    # Create GRUCell with dropout.\n",
    "#     encoder_cell = tf.contrib.rnn.GRUCell(hidden_size)######### YOUR CODE HERE #############\n",
    "#     tf.contrib.rnn.DropoutWrapper(encoder_cell, input_keep_prob=self.dropout_ph)\n",
    "    encoder_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(hidden_size), input_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    # Create RNN with the predefined cell.　编码完之后返回最后一个状态,运行encoder_cell\n",
    "    _, self.final_encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell, \n",
    "                                                    inputs=self.input_batch_embedded, \n",
    "                                                    sequence_length=self.input_batch_lengths, \n",
    "                                                    dtype=tf.float32)######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_encoder = classmethod(build_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The second RNN is called a *decoder* and serves for generating the output sequence. In the simple seq2seq arcitecture, the input sequence is provided to the decoder only as the final state of the encoder. Obviously, it is a bottleneck and [Attention techniques](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) can help to overcome it. So far, we do not need them to make our calculator work, but this would be a necessary ingredient for more advanced tasks. \n",
    "\n",
    "During training, decoder also uses information about the true output. It is feeded in as input symbol by symbol. However, during the prediction stage (which is called *inference* in this architecture), the decoder can only use its own generated output from the previous step to feed it in at the next step. Because of this difference (*training* vs *inference*), we will create two distinct instances, which will serve for the described scenarios.\n",
    "\n",
    "The picture below illustrates the point. It also shows our work with the special characters, e.g. look how the start symbol `^` is used. The transparent parts are ignored. In decoder, it is masked out in the loss computation. In encoder, the green state is considered as final and passed to the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    仅仅对编码器的最后一个状态进行解码．attention机制可以协助解决这个问题\\n    在训练期间，解码器可以使用真实的输出信息，把真实结果信息像输入特征一样喂进行就可以了．在测试期间，解码器就仅仅只能用它自己前一步输出的信息了\\n    因为训练和预测是不一样的，我们将会创建两种不同的实例，分别用于描述的两种场景．\\n    针对特定字符的工作：\"$\",在编码器中，$之后的信息就不会再进行计算，$所在的状态会传递到解码器\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    仅仅对编码器的最后一个状态进行解码．attention机制可以协助解决这个问题\n",
    "    在训练期间，解码器可以使用真实的输出信息，把真实结果信息像输入特征一样喂进行就可以了．在测试期间，解码器就仅仅只能用它自己前一步输出的信息了\n",
    "    因为训练和预测是不一样的，我们将会创建两种不同的实例，分别用于描述的两种场景．\n",
    "    针对特定字符的工作：\"$\",在编码器中，$之后的信息就不会再进行计算，$所在的状态会传递到解码器\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"encoder-decoder-pic.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to implement the decoder:\n",
    " - First, we should create two [helpers](https://www.tensorflow.org/api_guides/python/contrib.seq2seq#Dynamic_Decoding). These classes help to determine the behaviour of the decoder. During the training time, we will use [TrainingHelper](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper). For the inference we recommend to use [GreedyEmbeddingHelper](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper).\n",
    " - To share all parameters during training and inference, we use one scope and set the flag 'reuse' to True at inference time. You might be interested to know more about how [variable scopes](https://www.tensorflow.org/programmers_guide/variables) work in TF. \n",
    " - To create the decoder itself, we will use [BasicDecoder](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder) class. As previously, you should choose some RNN cell, e.g. GRU cell. To turn hidden states into logits, we will need a projection layer. One of the simple solutions is using [OutputProjectionWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/OutputProjectionWrapper).\n",
    " - For getting the predictions, it will be convinient to use [dynamic_decode](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode). This function uses the provided decoder to perform decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhelper:\\n    提供在解码过程中的抽样方法，例如训练过程中解码器的输出采用argmax算法还是广义伯努利分布算法，推断过程中输出采用argmax算法还是广义\\n    伯努利分布采样来得到输出id\\nGreedyEmbeddingHelper:\\n    采取argmax抽样算法来得到输出id,并且经过embedding层作为下一时刻的输入;\\nTrainingHelper:\\n    在sample过程中，它采用最简单的argmax算法．\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "helper:\n",
    "    提供在解码过程中的抽样方法，例如训练过程中解码器的输出采用argmax算法还是广义伯努利分布算法，推断过程中输出采用argmax算法还是广义\n",
    "    伯努利分布采样来得到输出id\n",
    "GreedyEmbeddingHelper:\n",
    "    采取argmax抽样算法来得到输出id,并且经过embedding层作为下一时刻的输入;\n",
    "TrainingHelper:\n",
    "    在sample过程中，它采用最简单的argmax算法．\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "    \"\"\"Specifies decoder architecture and computes the output.\n",
    "    \n",
    "        Uses different helpers:\n",
    "          - for train: feeding ground truth\n",
    "          - for inference: feeding generated output\n",
    "\n",
    "        As a result, self.train_outputs and self.infer_outputs are created. \n",
    "        Each of them contains two fields:\n",
    "          rnn_output (predicted logits)\n",
    "          sample_id (predictions).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use start symbols as the decoder inputs at the first time step.\n",
    "    batch_size = tf.shape(self.input_batch)[0]  #多少行文本，例如batch_size　大小为128\n",
    "    print(\"batch_size:\", batch_size)\n",
    "    # tf.fill(dims, value) 创建一个张量填充指定的常数\n",
    "    start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "    print(\"start_tokens:\", start_tokens)\n",
    "    print(\"start_symbol_id:\", start_symbol_id)\n",
    "    # tf.expand_dims(t,1) 给ｔ增加1维，最后一个参数１表示连接的维度\n",
    "    ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)  \n",
    "    \n",
    "    # Use the embedding layer defined before to lookup embedings for ground_truth_as_input. \n",
    "    self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings, ground_truth_as_input)######### YOUR CODE HERE #############\n",
    "     \n",
    "    # Create TrainingHelper for the train stage.\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded, \n",
    "                                                     self.ground_truth_lengths)\n",
    "    \n",
    "    # Create GreedyEmbeddingHelper for the inference stage.\n",
    "    # You should provide the embedding layer, start_tokens and index of the end symbol.\n",
    "    ######### YOUR CODE HERE #############\n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=self.embeddings, \n",
    "                                                            start_tokens=start_tokens,\n",
    "                                                            end_token=end_symbol_id)\n",
    "  \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "            #decoder_cell = tf.contrib.rnn.GRUCell(num_units=hidden_size, reuse=reuse)######### YOUR CODE HERE #############\n",
    "            \"\"\"\n",
    "                在rnn中使用dropout的方法和cnn不同，在rnn中进行dropout时，对于rnn的部分不进行dropout，也就是说t-1时刻的状态传递\n",
    "                到t时刻进行计算时，这个中间不进行memory的dropout;仅在同一t时刻中，多层cell之间传递信息的时候进行dropout.\n",
    "                注：Dropout只能是层与层之间的Dropout，同一个层里面，T时刻与T+1时刻是不会Dropout的\n",
    "            \"\"\"\n",
    "            #tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(num_units=hidden_size, reuse=reuse), input_keep_prob=self.dropout_ph)\n",
    "            \n",
    "            # Create a projection wrapper.\n",
    "            decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "            #print(\"decoder_cell.shape:\", decoder_cell.shape)\n",
    "            \n",
    "            # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "            # The initial state should be equal to the final state of the encoder!\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                                     helper=helper,\n",
    "                                                     initial_state=self.final_encoder_state)######### YOUR CODE HERE #############\n",
    "            \n",
    "            # The first returning argument of dynamic_decode contains two fields:\n",
    "            #   rnn_output (predicted logits)  原来我要的只是这个rnn_output\n",
    "            #   sample_id (predictions)\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, \n",
    "                                                              maximum_iterations=max_iter, \n",
    "                                                              output_time_major=False, \n",
    "                                                              impute_finished=True)\n",
    "            return outputs\n",
    "        \n",
    "    self.train_outputs = decode(train_helper, 'decode')\n",
    "    self.infer_outputs = decode(infer_helper, 'decode', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_decoder = classmethod(build_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will use [sequence_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss), which is a weighted cross-entropy loss for a sequence of logits. Take a moment to understand, what is your train logits and targets. Also note, that we do not want to take into account loss terms coming from padding symbols, so we will mask them out using weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "    \n",
    "    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32) #真实的标签序列的权重\n",
    "    \n",
    "    ######### YOUR CODE HERE #############\n",
    "    print(\"self.train_outputs.shape:\", self.train_outputs)\n",
    "    #tf.contrib.seq2seq.sequence_loss() 序列的加权交叉熵损失\n",
    "    print(\"self.train_outputs.rnn_output.shape:\", self.train_outputs.rnn_output)\n",
    "    print(\"self.ground_truth.shape:\", self.ground_truth.shape)\n",
    "    \"\"\"\n",
    "    with ops.name_scope(name, \"sequence_loss\", [logits, targets, weights]):\n",
    "        num_classes = array_ops.shape(logits)[2]\n",
    "        logits_flat = array_ops.reshape(logits, [-1, num_classes])\n",
    "        targets = array_ops.reshape(targets, [-1])\n",
    "        if softmax_loss_function is None:\n",
    "            crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits_flat)\n",
    "    由这段代码来看，logits 和 target的前两维度必须相同．\n",
    "    \"\"\"\n",
    "    #默认是平均权重的交叉熵损失\n",
    "    self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                                                 self.train_outputs.rnn_output, \n",
    "                                                 self.ground_truth, \n",
    "                                                 weights \n",
    "                                                 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to specify is the optimization of the defined loss. \n",
    "We suggest that you use [optimize_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss) with Adam optimizer and a learning rate from the corresponding placeholder. You might also need to pass global step (e.g. as tf.train.get_global_step()) and clip gradients by 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies train_op that optimizes self.loss.\"\"\"\n",
    "    \n",
    "    ######### YOUR CODE HERE #############\n",
    "    self.train_op = tf.contrib.layers.optimize_loss(loss=self.loss, \n",
    "                                                    global_step=tf.train.get_global_step(), \n",
    "                                                    learning_rate=self.learning_rate_ph, \n",
    "                                                    optimizer=\"Adam\", \n",
    "                                                    clip_gradients=1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have specified all the parts of your network. You may have noticed, that we didn't deal with any real data yet, so what you have written is just recipies on how the network should function.\n",
    "Now we will put them to the constructor of our Seq2SeqModel class to use it in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    至此，已经完成了我的网络中的所有的部分，下一章节中将会把这个网络放进seq2seq模型\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    至此，已经完成了我的网络中的所有的部分，下一章节中将会把这个网络放进seq2seq模型\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这个函数主要是进行一些初始化操作\n",
    "def init_model(self, vocab_size, embeddings_size, hidden_size, \n",
    "               max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "    \n",
    "    self.__declare_placeholders()\n",
    "    self.__create_embeddings(vocab_size, embeddings_size)\n",
    "    self.__build_encoder(hidden_size)\n",
    "    self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "    \n",
    "    # Compute loss and back-propagate.\n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()\n",
    "    \n",
    "    # Get predictions for evaluation.\n",
    "    self.train_predictions = self.train_outputs.sample_id   #取某个对象的某个字段，直接\".\"就可以了\n",
    "    self.infer_predictions = self.infer_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict output\n",
    "\n",
    "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*. To predict output, we just need to compute *self.infer_predictions*. In any case, we need to feed actual data through the placeholders that we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "    #原来self.input_batch 和 self.ground_truth　一个指的是样本集中的x，一个指的是样本集中的y\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "    \n",
    "    pred, loss, _ = session.run(\n",
    "            [\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op\n",
    "            ], \n",
    "            feed_dict=feed_dict\n",
    "    )\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented two prediction functions: *predict_for_batch* and *predict_for_batch_with_loss*. The first one allows only to predict output for some input sequence, while the second one could compute loss because we provide also ground truth values. Both these functions might be useful since the first one could be used for predicting only, and the second one is helpful for validating results on not-training data during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    predict_for_batch:仅仅会为部分输入序列预测输出\\n    predice_for_batch_with_loss:这个还可以计算损失，因为提供了ground truth 的值\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    predict_for_batch:仅仅会为部分输入序列预测输出\n",
    "    predice_for_batch_with_loss:这个还可以计算损失，因为提供了ground truth 的值\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, X, X_seq_len):\n",
    "    ######### YOUR CODE HERE #############\n",
    "    feed_dict = {\n",
    "        self.input_batch:X,\n",
    "        self.input_batch_lengths:X_seq_len\n",
    "    }\n",
    "    \n",
    "    pred = session.run(\n",
    "            [self.infer_predictions], \n",
    "            feed_dict=feed_dict\n",
    "        )[0]\n",
    "    return pred\n",
    "\n",
    "def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "    ######### YOUR CODE HERE #############\n",
    "    feed_dict = {\n",
    "        self.input_batch:X,\n",
    "        self.input_batch_lengths:X_seq_len,\n",
    "        self.ground_truth:Y,\n",
    "        self.ground_truth_lengths:Y_seq_len\n",
    "    }\n",
    "    pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Seq2SeqModel.predict_for_batch = classmethod(predict_for_batch)\n",
    "Seq2SeqModel.predict_for_batch_with_loss = classmethod(predict_for_batch_with_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiment\n",
    "\n",
    "Create *Seq2SeqModel* model with the following parameters:\n",
    " - *vocab_size* — number of tokens;\n",
    " - *embeddings_size* — dimension of embeddings, recommended value: 20;\n",
    " - *max_iter* — maximum number of steps in decoder, recommended value: 7;\n",
    " - *hidden_size* — size of hidden layers for RNN, recommended value: 512;\n",
    " - *start_symbol_id* — an index of the start token (`^`).\n",
    " - *end_symbol_id* — an index of the end token (`$`).\n",
    " - *padding_symbol_id* — an index of the padding token (`#`).\n",
    "\n",
    "Set hyperparameters. You might want to start with the following values and see how it works:\n",
    "- *batch_size*: 128;\n",
    "- at least 10 epochs;\n",
    "- value of *learning_rate*: 0.001\n",
    "- *dropout_keep_probability* equals to 0.5 for training (typical values for dropout probability are ranging from 0.1 to 1.0); larger values correspond smaler number of dropout units;\n",
    "- *max_len*: 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "start_tokens: Tensor(\"Fill:0\", shape=(?,), dtype=int32)\n",
      "start_symbol_id: 0\n",
      "self.train_outputs.shape: BasicDecoderOutput(rnn_output=<tf.Tensor 'decode/decoder/transpose:0' shape=(?, ?, 15) dtype=float32>, sample_id=<tf.Tensor 'decode/decoder/transpose_1:0' shape=(?, ?) dtype=int32>)\n",
      "self.train_outputs.rnn_output.shape: Tensor(\"decode/decoder/transpose:0\", shape=(?, ?, 15), dtype=float32)\n",
      "self.ground_truth.shape: (?, ?)\n"
     ]
    }
   ],
   "source": [
    "#init_model(self, vocab_size, embeddings_size, hidden_size,max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = Seq2SeqModel(vocab_size=15, \n",
    "                     embeddings_size=20, \n",
    "                     hidden_size=512, \n",
    "                     max_iter=7, \n",
    "                     start_symbol_id=word2id[\"^\"], \n",
    "                     end_symbol_id=word2id[\"$\"], \n",
    "                     padding_symbol_id=word2id[\"#\"])######### YOUR CODE HERE #############\n",
    "\n",
    "batch_size = 128######### YOUR CODE HERE #############\n",
    "n_epochs = 10######### YOUR CODE HERE #############\n",
    "learning_rate = 0.001######### YOUR CODE HERE #############\n",
    "dropout_keep_probability = 0.5######### YOUR CODE HERE #############\n",
    "max_len = 20######### YOUR CODE HERE #############\n",
    "\n",
    "n_step = int(len(train_set) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the training! A good indicator that everything works fine is decreasing loss during the training. You should account on the loss value equal to approximately 2.7 at the beginning of the training and near 1 after the 10th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    现在已经到了最后一步了，准备好运行这个训练，所有的步骤都运行得很好的一个表现就是损失函数一直在下降，刚开始的时候，损失函数的值可能是\\n    2.7, 运行完10个epoch之后，损失函数的值大概可以下降到1.0的样子\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    现在已经到了最后一步了，准备好运行这个训练，所有的步骤都运行得很好的一个表现就是损失函数一直在下降，刚开始的时候，损失函数的值可能是\n",
    "    2.7, 运行完10个epoch之后，损失函数的值大概可以下降到1.0的样子\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n",
      "(1, 4, 4)\n",
      "(2, 5, 5)\n",
      "(3, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "#zip函数的用法\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [4,5,6,7,8]\n",
    "zipped = zip(a,b)     # 打包为元组的列表\n",
    "for z in zipped:\n",
    "    print(z)\n",
    "for z in zip(a,b,c):\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/10], step: [1/625], loss: 2.712346\n",
      "Epoch: [1/10], step: [201/625], loss: 1.841499\n",
      "Epoch: [1/10], step: [401/625], loss: 1.756556\n",
      "Epoch: [1/10], step: [601/625], loss: 1.691974\n",
      "Test: epoch 1 loss: 1.60785\n",
      "X: 6848+6299$\n",
      "Y: 13147$\n",
      "O: 12916$\n",
      "\n",
      "X: 7868-1290$\n",
      "Y: 6578$#\n",
      "O: 6659$^\n",
      "\n",
      "X: 7184+5281$\n",
      "Y: 12465$\n",
      "O: 11981$\n",
      "\n",
      "X: 1270+5193$\n",
      "Y: 6463$#\n",
      "O: 6116$^\n",
      "\n",
      "X: 6552+8342$\n",
      "Y: 14894$\n",
      "O: 12919$\n",
      "\n",
      "X: 7137-4608$\n",
      "Y: 2529$#\n",
      "O: 2944$^\n",
      "\n",
      "X: 610+2051$#\n",
      "Y: 2661$#\n",
      "O: 5119$^\n",
      "\n",
      "X: 9976+184$#\n",
      "Y: 10160$\n",
      "O: 9989$^\n",
      "\n",
      "X: 7848-9747$\n",
      "Y: -1899$\n",
      "O: -194$^\n",
      "\n",
      "X: 9375-8483$\n",
      "Y: 892$##\n",
      "O: 1194$^\n",
      "\n",
      "Train: epoch 2\n",
      "Epoch: [2/10], step: [1/625], loss: 1.661276\n",
      "Epoch: [2/10], step: [201/625], loss: 1.557394\n",
      "Epoch: [2/10], step: [401/625], loss: 1.519405\n",
      "Epoch: [2/10], step: [601/625], loss: 1.513980\n",
      "Test: epoch 2 loss: 1.43447\n",
      "X: 3757-4201$\n",
      "Y: -444$#\n",
      "O: -1155$\n",
      "\n",
      "X: 4775+8214$\n",
      "Y: 12989$\n",
      "O: 13133$\n",
      "\n",
      "X: 282-7554$#\n",
      "Y: -7272$\n",
      "O: -7933$\n",
      "\n",
      "X: 8384-2877$\n",
      "Y: 5507$#\n",
      "O: 5918$^\n",
      "\n",
      "X: 7327+6837$\n",
      "Y: 14164$\n",
      "O: 13813$\n",
      "\n",
      "X: 5952+3259$\n",
      "Y: 9211$#\n",
      "O: 9555$^\n",
      "\n",
      "X: 5035-5198$\n",
      "Y: -163$#\n",
      "O: -1119$\n",
      "\n",
      "X: 3202+1164$\n",
      "Y: 4366$#\n",
      "O: 4433$^\n",
      "\n",
      "X: 2336+2138$\n",
      "Y: 4474$#\n",
      "O: 4433$^\n",
      "\n",
      "X: 1223-8783$\n",
      "Y: -7560$\n",
      "O: -7933$\n",
      "\n",
      "Train: epoch 3\n",
      "Epoch: [3/10], step: [1/625], loss: 1.517449\n",
      "Epoch: [3/10], step: [201/625], loss: 1.434963\n",
      "Epoch: [3/10], step: [401/625], loss: 1.421942\n",
      "Epoch: [3/10], step: [601/625], loss: 1.403872\n",
      "Test: epoch 3 loss: 1.38199\n",
      "X: 8723+5048$\n",
      "Y: 13771$\n",
      "O: 13444$\n",
      "\n",
      "X: 2969+1428$\n",
      "Y: 4397$#\n",
      "O: 4000$^\n",
      "\n",
      "X: 2558+3660$\n",
      "Y: 6218$#\n",
      "O: 6000$^\n",
      "\n",
      "X: 657+4171$#\n",
      "Y: 4828$#\n",
      "O: 4980$^\n",
      "\n",
      "X: 6364-5522$\n",
      "Y: 842$##\n",
      "O: 1044$^\n",
      "\n",
      "X: 1969+9865$\n",
      "Y: 11834$\n",
      "O: 12044$\n",
      "\n",
      "X: 6365+6208$\n",
      "Y: 12573$\n",
      "O: 12904$\n",
      "\n",
      "X: 6996-5247$\n",
      "Y: 1749$#\n",
      "O: 1764$^\n",
      "\n",
      "X: 8072+6013$\n",
      "Y: 14085$\n",
      "O: 14244$\n",
      "\n",
      "X: 1258-7640$\n",
      "Y: -6382$\n",
      "O: -6000$\n",
      "\n",
      "Train: epoch 4\n",
      "Epoch: [4/10], step: [1/625], loss: 1.443030\n",
      "Epoch: [4/10], step: [201/625], loss: 1.371681\n",
      "Epoch: [4/10], step: [401/625], loss: 1.412449\n",
      "Epoch: [4/10], step: [601/625], loss: 1.356904\n",
      "Test: epoch 4 loss: 1.3222\n",
      "X: 7386-7933$\n",
      "Y: -547$#\n",
      "O: -1077$\n",
      "\n",
      "X: 1734-1045$\n",
      "Y: 689$##\n",
      "O: 1077$^\n",
      "\n",
      "X: 7276-1955$\n",
      "Y: 5321$#\n",
      "O: 5225$^\n",
      "\n",
      "X: 472+1555$#\n",
      "Y: 2027$#\n",
      "O: 1985$^\n",
      "\n",
      "X: 6909-9722$\n",
      "Y: -2813$\n",
      "O: -3292$\n",
      "\n",
      "X: 3525+128$#\n",
      "Y: 3653$#\n",
      "O: 3829$^\n",
      "\n",
      "X: 1676+7246$\n",
      "Y: 8922$#\n",
      "O: 9011$^\n",
      "\n",
      "X: 8049-9835$\n",
      "Y: -1786$\n",
      "O: -1882$\n",
      "\n",
      "X: 6230+5217$\n",
      "Y: 11447$\n",
      "O: 11955$\n",
      "\n",
      "X: 7182+5313$\n",
      "Y: 12495$\n",
      "O: 12922$\n",
      "\n",
      "Train: epoch 5\n",
      "Epoch: [5/10], step: [1/625], loss: 1.362315\n",
      "Epoch: [5/10], step: [201/625], loss: 1.329202\n",
      "Epoch: [5/10], step: [401/625], loss: 1.316679\n",
      "Epoch: [5/10], step: [601/625], loss: 1.302939\n",
      "Test: epoch 5 loss: 1.28939\n",
      "X: 5406+976$#\n",
      "Y: 6382$#\n",
      "O: 6477$^\n",
      "\n",
      "X: 8783+2363$\n",
      "Y: 11146$\n",
      "O: 11000$\n",
      "\n",
      "X: 9144+7891$\n",
      "Y: 17035$\n",
      "O: 16788$\n",
      "\n",
      "X: 7378-9394$\n",
      "Y: -2016$\n",
      "O: -2100$\n",
      "\n",
      "X: 4710+1887$\n",
      "Y: 6597$#\n",
      "O: 6777$^\n",
      "\n",
      "X: 6986-3322$\n",
      "Y: 3664$#\n",
      "O: 3782$^\n",
      "\n",
      "X: 866+9445$#\n",
      "Y: 10311$\n",
      "O: 10477$\n",
      "\n",
      "X: 5418+2848$\n",
      "Y: 8266$#\n",
      "O: 8447$^\n",
      "\n",
      "X: 9177-5917$\n",
      "Y: 3260$#\n",
      "O: 2982$^\n",
      "\n",
      "X: 3262+2431$\n",
      "Y: 5693$#\n",
      "O: 5777$^\n",
      "\n",
      "Train: epoch 6\n",
      "Epoch: [6/10], step: [1/625], loss: 1.329425\n",
      "Epoch: [6/10], step: [201/625], loss: 1.319428\n",
      "Epoch: [6/10], step: [401/625], loss: 1.280790\n",
      "Epoch: [6/10], step: [601/625], loss: 1.304578\n",
      "Test: epoch 6 loss: 1.24118\n",
      "X: 5898+7942$\n",
      "Y: 13840$\n",
      "O: 13545$\n",
      "\n",
      "X: 5864+1025$\n",
      "Y: 6889$#\n",
      "O: 6935$^\n",
      "\n",
      "X: 9580-2842$\n",
      "Y: 6738$#\n",
      "O: 6907$^\n",
      "\n",
      "X: 9763-9015$\n",
      "Y: 748$##\n",
      "O: 511$^^\n",
      "\n",
      "X: 3338+6698$\n",
      "Y: 10036$\n",
      "O: 10083$\n",
      "\n",
      "X: 2892+1735$\n",
      "Y: 4627$#\n",
      "O: 4919$^\n",
      "\n",
      "X: 3283-5966$\n",
      "Y: -2683$\n",
      "O: -3275$\n",
      "\n",
      "X: 3133-7130$\n",
      "Y: -3997$\n",
      "O: -3835$\n",
      "\n",
      "X: 7403-4250$\n",
      "Y: 3153$#\n",
      "O: 3075$^\n",
      "\n",
      "X: 2399-1951$\n",
      "Y: 448$##\n",
      "O: 417$^^\n",
      "\n",
      "Train: epoch 7\n",
      "Epoch: [7/10], step: [1/625], loss: 1.296861\n",
      "Epoch: [7/10], step: [201/625], loss: 1.258299\n",
      "Epoch: [7/10], step: [401/625], loss: 1.222392\n",
      "Epoch: [7/10], step: [601/625], loss: 1.169688\n",
      "Test: epoch 7 loss: 1.1453\n",
      "X: 4681+6018$\n",
      "Y: 10699$\n",
      "O: 10766$\n",
      "\n",
      "X: 1161-931$#\n",
      "Y: 230$##\n",
      "O: 470$^^\n",
      "\n",
      "X: 6787+8858$\n",
      "Y: 15645$\n",
      "O: 15593$\n",
      "\n",
      "X: 9329+2956$\n",
      "Y: 12285$\n",
      "O: 12266$\n",
      "\n",
      "X: 4818+7917$\n",
      "Y: 12735$\n",
      "O: 12777$\n",
      "\n",
      "X: 5248+7610$\n",
      "Y: 12858$\n",
      "O: 12766$\n",
      "\n",
      "X: 6614-6177$\n",
      "Y: 437$##\n",
      "O: 636$^^\n",
      "\n",
      "X: 2492-1809$\n",
      "Y: 683$##\n",
      "O: 699$^^\n",
      "\n",
      "X: 2682-9584$\n",
      "Y: -6902$\n",
      "O: -6016$\n",
      "\n",
      "X: 9767+5455$\n",
      "Y: 15222$\n",
      "O: 14291$\n",
      "\n",
      "Train: epoch 8\n",
      "Epoch: [8/10], step: [1/625], loss: 1.175353\n",
      "Epoch: [8/10], step: [201/625], loss: 1.109932\n",
      "Epoch: [8/10], step: [401/625], loss: 1.100573\n",
      "Epoch: [8/10], step: [601/625], loss: 1.080790\n",
      "Test: epoch 8 loss: 1.01146\n",
      "X: 7332+9726$\n",
      "Y: 17058$\n",
      "O: 16106$\n",
      "\n",
      "X: 8504-7694$\n",
      "Y: 810$##\n",
      "O: 1009$^\n",
      "\n",
      "X: 8253-9340$\n",
      "Y: -1087$\n",
      "O: -1100$\n",
      "\n",
      "X: 8128-3$###\n",
      "Y: 8125$#\n",
      "O: 8204$^\n",
      "\n",
      "X: 5826+1385$\n",
      "Y: 7211$#\n",
      "O: 7200$^\n",
      "\n",
      "X: 7662+6174$\n",
      "Y: 13836$\n",
      "O: 13830$\n",
      "\n",
      "X: 1323-5221$\n",
      "Y: -3898$\n",
      "O: -3884$\n",
      "\n",
      "X: 7044-4072$\n",
      "Y: 2972$#\n",
      "O: 3004$^\n",
      "\n",
      "X: 3634-7868$\n",
      "Y: -4234$\n",
      "O: -4193$\n",
      "\n",
      "X: 2661+4632$\n",
      "Y: 7293$#\n",
      "O: 7343$^\n",
      "\n",
      "Train: epoch 9\n",
      "Epoch: [9/10], step: [1/625], loss: 1.065107\n",
      "Epoch: [9/10], step: [201/625], loss: 1.043650\n",
      "Epoch: [9/10], step: [401/625], loss: 1.019467\n",
      "Epoch: [9/10], step: [601/625], loss: 0.989118\n",
      "Test: epoch 9 loss: 0.994903\n",
      "X: 7110-2452$\n",
      "Y: 4658$#\n",
      "O: 4662$^\n",
      "\n",
      "X: 6238-6067$\n",
      "Y: 171$##\n",
      "O: 184$^^\n",
      "\n",
      "X: 8342+4865$\n",
      "Y: 13207$\n",
      "O: 13244$\n",
      "\n",
      "X: 7456+3945$\n",
      "Y: 11401$\n",
      "O: 11404$\n",
      "\n",
      "X: 4881+2631$\n",
      "Y: 7512$#\n",
      "O: 7523$^\n",
      "\n",
      "X: 9255+9371$\n",
      "Y: 18626$\n",
      "O: 18688$\n",
      "\n",
      "X: 3800+6097$\n",
      "Y: 9897$#\n",
      "O: 9944$^\n",
      "\n",
      "X: 1966-5351$\n",
      "Y: -3385$\n",
      "O: -3377$\n",
      "\n",
      "X: 1571+2707$\n",
      "Y: 4278$#\n",
      "O: 4377$^\n",
      "\n",
      "X: 9931+8265$\n",
      "Y: 18196$\n",
      "O: 18244$\n",
      "\n",
      "Train: epoch 10\n",
      "Epoch: [10/10], step: [1/625], loss: 1.009824\n",
      "Epoch: [10/10], step: [201/625], loss: 1.003282\n",
      "Epoch: [10/10], step: [401/625], loss: 0.972713\n",
      "Epoch: [10/10], step: [601/625], loss: 1.005627\n",
      "Test: epoch 10 loss: 0.9181\n",
      "X: 235+798$##\n",
      "Y: 1033$#\n",
      "O: 1123$^\n",
      "\n",
      "X: 3069-3130$\n",
      "Y: -61$##\n",
      "O: -14$^^\n",
      "\n",
      "X: 542+4474$#\n",
      "Y: 5016$#\n",
      "O: 5027$^\n",
      "\n",
      "X: 521+4712$#\n",
      "Y: 5233$#\n",
      "O: 5226$^\n",
      "\n",
      "X: 2808-2601$\n",
      "Y: 207$##\n",
      "O: 215$^^\n",
      "\n",
      "X: 7881+1342$\n",
      "Y: 9223$#\n",
      "O: 9200$^\n",
      "\n",
      "X: 3985+2496$\n",
      "Y: 6481$#\n",
      "O: 6439$^\n",
      "\n",
      "X: 1064-527$#\n",
      "Y: 537$##\n",
      "O: 666$^^\n",
      "\n",
      "X: 6048+2090$\n",
      "Y: 8138$#\n",
      "O: 8199$^\n",
      "\n",
      "X: 4088-1701$\n",
      "Y: 2387$#\n",
      "O: 2388$^\n",
      "\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())    #全局初始化\n",
    "            \n",
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):  \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    print('Train: epoch', epoch + 1)\n",
    "    #每一次遍历训练集，都分成batch_size个批次进行\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        \n",
    "        # prepare the data (X_batch and Y_batch) for training\n",
    "        # using function batch_to_ids\n",
    "        X, X_seq_len = batch_to_ids(X_batch, word2id, max_len=20) \n",
    "        Y, Y_seq_len = batch_to_ids(Y_batch, word2id, max_len=20) \n",
    "        #print(\"Y.shape:\", len(Y), len(Y[0]))    128*20\n",
    "        #print(\"X.shape:\", len(X), len(X[0]))\n",
    "        predictions, loss = model.train_on_batch(session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "\n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    # prepare test data (X_sent and Y_sent) for predicting \n",
    "    # quality and computing value of the loss function\n",
    "    # using function batch_to_ids\n",
    "    X, X_seq_len = batch_to_ids(X_sent, word2id, max_len=20) \n",
    "    Y, Y_seq_len = batch_to_ids(Y_sent, word2id, max_len=20)  \n",
    "    \n",
    "    ######### YOUR CODE HERE #############\n",
    "    predictions, loss = model.predict_for_batch_with_loss(session,  X, X_seq_len, Y, Y_seq_len)\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss,)\n",
    "    \n",
    "    #zip() 每个迭代对象的第i个元素组合成一个tuple,有几个迭代对象，tuple里面就有几个元素，共len(最短迭代对象个tuple)\n",
    "    for x, y, p  in list(zip(X, Y, predictions))[:10]:    #可能会报错\n",
    "        print('X:',''.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:',''.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:',''.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "\n",
    "    model_predictions = []\n",
    "    ground_truth = []\n",
    "    invalid_number_prediction_count = 0\n",
    "    # For the whole test set calculate ground-truth values (as integer numbers)\n",
    "    # and prediction values (also as integers) to calculate metrics.\n",
    "    # If generated by model number is not correct (e.g. '1-1'), \n",
    "    # increase invalid_number_prediction_count and don't append this and corresponding\n",
    "    # ground-truth value to the arrays.\n",
    "    for X_batch, Y_batch in generate_batches(test_set, batch_size=batch_size):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "\n",
    "        X, X_seq_len = batch_to_ids(X_batch, word2id, max_len=20) \n",
    "        Y, Y_seq_len = batch_to_ids(Y_batch, word2id, max_len=20) \n",
    "        predictions = model.predict_for_batch(session, X, X_seq_len) #pred is a ndarray\n",
    "        \n",
    "        #是判断错误形式的答案(直接模仿上面的循环就可以)\n",
    "        #预测值和真实值是否一致，那是后面的MAE做的事情\n",
    "        for y, p, y_len in list(zip(Y, predictions, Y_seq_len)):\n",
    "            true_y = \"\".join(ids_to_sentence(y, id2word))\n",
    "            pred_y = \"\".join(ids_to_sentence(p, id2word))\n",
    "            try:\n",
    "                stop_idx = pred_y.find(\"$\")\n",
    "                if stop_idx != -1:\n",
    "                    prediction = int(pred_y[:stop_idx])\n",
    "                else:\n",
    "                    prediction = int(pred_y)\n",
    "                model_predictions.append(prediction)\n",
    "                ground_truth.append(int(true_y[:y_len-1]))\n",
    "            except ValueError:\n",
    "                invalid_number_prediction_count += 1\n",
    "               \n",
    "                \n",
    "                \n",
    "    all_model_predictions.append(model_predictions)\n",
    "    all_ground_truth.append(ground_truth)\n",
    "    invalid_number_prediction_counts.append(invalid_number_prediction_count)\n",
    "            \n",
    "print('\\n...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "Because our task is simple and the output is straight-forward, we will use [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) metric to evaluate the trained model during the epochs. Compute the value of the metric for the output from each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, MAE: 1038.991350, Invalid numbers: 0\n",
      "Epoch: 2, MAE: 466.554350, Invalid numbers: 0\n",
      "Epoch: 3, MAE: 289.825850, Invalid numbers: 0\n",
      "Epoch: 4, MAE: 235.246850, Invalid numbers: 0\n",
      "Epoch: 5, MAE: 179.358600, Invalid numbers: 0\n",
      "Epoch: 6, MAE: 165.955500, Invalid numbers: 0\n",
      "Epoch: 7, MAE: 99.184600, Invalid numbers: 0\n",
      "Epoch: 8, MAE: 63.348700, Invalid numbers: 0\n",
      "Epoch: 9, MAE: 52.505700, Invalid numbers: 0\n",
      "Epoch: 10, MAE: 35.521250, Invalid numbers: 0\n"
     ]
    }
   ],
   "source": [
    "#enumerate的作用仅仅是为每个元素的顺序执行添加了一个对应的序号，仅此而已\n",
    "for i, (gts, predictions, invalid_number_prediction_count) in enumerate(zip(all_ground_truth,\n",
    "                                                                            all_model_predictions,\n",
    "                                                                            invalid_number_prediction_counts), 1):\n",
    "    mae = mean_absolute_error(y_pred=predictions,y_true=gts)  ######### YOUR CODE HERE #############\n",
    "    print(\"Epoch: %i, MAE: %f, Invalid numbers: %i\" % (i, mae, invalid_number_prediction_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 2018.7.2 完成，耗时两天\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
