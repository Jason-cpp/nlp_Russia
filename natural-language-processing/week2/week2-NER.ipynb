{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "用Bi-LSTMS来进行解决\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
    "\n",
    "### Libraries\n",
    "\n",
    "For this task you will need the following libraries:\n",
    " - [Tensorflow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
    " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    " \n",
    "If you have never worked with Tensorflow, you would probably need to read some tutorials during your work on this assignment, e.g. [this one](https://www.tensorflow.org/tutorials/recurrent) could be a good starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week2/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    课程设计：识别twitter中的命名实体\\n    数据量：约13万条数据\\n    采用的网络：双向LSTM\\n    准确率：\\n    召回率：\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    课程设计：识别twitter中的命名实体\n",
    "    数据量：约13万条数据\n",
    "    采用的网络：双向LSTM\n",
    "    准确率：\n",
    "    召回率：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.txt is already downloaded.\n",
      "File data/validation.txt is already downloaded.\n",
      "File data/test.txt is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    用递归神经网络解决命名实体识别的问题．命名实体识别是自然语言系统中的一个常见的任务．可以用来提取人物，机构，地点等实体。\n",
    "    这个实验是从推特中识别出实体\n",
    "    如果我没有想错，这个命名实体识别的作业在面试的时候应该是可以拿出来讲的（算是NLP方面一个具体的任务）\n",
    "    解决方案是通过双向LSTM来解决\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week2_resources\n",
    "\n",
    "download_week2_resources()  #下载训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Twitter Named Entity Recognition corpus\n",
    "\n",
    "\n",
    "We will work with a corpus, which contains twits with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    语料中包含twitter和对应的标签．每一行包含一条twitter，每一个twitter有token/tag对组成，空格分隔，不同tweet之间有一个空行\n",
    "    \n",
    "    处理完毕之后返回一个token list 和 tag list\n",
    "    将user's nickname　用<USR>标签替换，nicknname一般以 @ 开始\n",
    "    将URL用<URL>标签替换，url一般以 https:// 或 http:// 开始\n",
    "\"\"\"\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:  #这里应该是指遇到空行的时候\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)     #应该是用append，形成一个列表的列表\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()   #已经按照空格进行切分了\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            ######################################\n",
    "            ######### YOUR CODE HERE #############\n",
    "            ######################################\n",
    "            if token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "                token = \"<URL>\"\n",
    "            if token.startswith(\"@\"):\n",
    "                token = \"<USR>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')  #用来训练模型\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')  #用来评估和调节超参数\n",
    "test_tokens, test_tags = read_data('data/test.txt')    #用来评估最终的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))   #切分正确\n",
    "        #print(type((token,tag)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    形成两种映射：\n",
    "        词->词id：便于为当前的词定位到词嵌入矩阵中特定的行\n",
    "        标签->标签id: 便于形成one-hot向量，便于为网络的输出计算损失\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    添加 token or tag 到下标的映射\n",
    "    read_data 将 用户昵称 和 超链接 替换完之后才放进 build_dict\n",
    "\"\"\"\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
    "    # Add special tokens (or tags) to the dictionaries.      将一些特殊的token或tags加入字典\n",
    "    # The first special token must have index 0.　　          第一个特殊的token必须为零\n",
    "    \n",
    "    # Mapping tok2idx should contain each token or tag only once. \n",
    "    # To do so, you should extract unique tokens/tags from the tokens_or_tags variable\n",
    "    # and then index them (for example, you can add them into the list idx2tok\n",
    "    # and for each token/tag save the index into tok2idx).\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    #利用set去重\n",
    "    tokens = []\n",
    "    uni_tokens = []\n",
    "    for token_oneTwitter in tokens_or_tags:\n",
    "        tokens.extend(token_oneTwitter)\n",
    "#     print(uni_tokens)\n",
    "   \n",
    "    #用于统计带标签的5000个高频词\n",
    "    withTag_tokens = []\n",
    "    for i in len(tokens_or_tags):\n",
    "        \n",
    "    \n",
    "    uni_tokens =list(set(tokens))   #利用set去重(看看序列长度，只能保证词表大小个单词_5000)\n",
    "    print(\"uni_tokens.length:\", len(uni_tokens)) #有20505个不重复的字符串，不可能都建立词向量，那怎么办，可以建高频词\n",
    "    \n",
    "    #取最高频的5000个词(对tokens取，而不是uni_tokens),改为收集带标签的5000个高频词，效果应该会提升很多\n",
    "    counter = Counter(tokens)     #这特么，去重了怎么统计高频词\n",
    "    count_pairs = counter.most_common(5000-len(special_tokens))\n",
    "    words,_ = list(zip(*count_pairs))  #4998个高频单词的集合\n",
    "    words = list(words)    #元组转list\n",
    "#     print(\"最高频的5000个词：\", words)\n",
    "     \n",
    "    #对于tag,首先要为标签'0'去重，这几行代码专为tag\n",
    "    if len(special_tokens)==1:\n",
    "        tag = special_tokens[0]\n",
    "        if tag in words:\n",
    "            words.remove('O')\n",
    "    #防止这个特殊tag早已经存在\n",
    "    words = special_tokens + list(words) #5000收集完毕（还得思考低频词如何处理）\n",
    "    \n",
    "    \n",
    "    #这里做一个判断，如果len(words)<50，就拼接到50，这一步是为了tags,不好做，tag映射成下标有问题\n",
    "#     if len(words)<50:\n",
    "#         for i in range(len(words),50):\n",
    "#             words.append(\"0\")\n",
    "    print(\"len(words):\", len(words))\n",
    "   \n",
    "    \n",
    "    #为每个token赋予一个下标\n",
    "    tok2idx = dict(zip(words, range(len(words) )))  #将两个长度相同的list按序打包成字典，前一个为键，后一个为值\n",
    "    idx2tok = {}\n",
    "    for key, val in tok2idx.items():  #字典反转，键值互换（首先要保证两者都是不重复的）\n",
    "        idx2tok[val] = key\n",
    "    print(\"len(tok2idx):\", len(tok2idx))\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni_tokens.length: 20503\n",
      "len(words): 5000\n",
      "len(tok2idx): 5000\n",
      "uni_tokens.length: 21\n",
      "len(words): 21\n",
      "len(tok2idx): 21\n",
      "tag2idx: {'O': 0, 'B-geo-loc': 1, 'B-person': 2, 'I-other': 3, 'B-other': 4, 'B-company': 5, 'I-product': 6, 'I-person': 7, 'I-facility': 8, 'B-product': 9, 'B-facility': 10, 'B-musicartist': 11, 'I-geo-loc': 12, 'I-company': 13, 'B-sportsteam': 14, 'I-musicartist': 15, 'I-movie': 16, 'I-sportsteam': 17, 'B-movie': 18, 'B-tvshow': 19, 'I-tvshow': 20}\n"
     ]
    }
   ],
   "source": [
    "#<UNK>表示未登录词\n",
    "#<PAD>用于填充句子，使长度相同\n",
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries （创建词映射和标签映射字典）\n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
    "print(\"tag2idx:\", tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    来了任何一个句子，都可以词转下标，标签转下标，或者是下标转词，下标转标签\n",
    "\"\"\"\n",
    "def words2idxs(tokens_list):     #如果不在我的词典中，直接赋值标签<UNK>,<UNK>对应的tag是\"O\"\n",
    "    idxs = []\n",
    "    for word in tokens_list:\n",
    "        if word in token2idx:\n",
    "            idxs.append(token2idx[word])\n",
    "        else:\n",
    "            idxs.append(0)    #0 比表示<UNK>对应的下标，说明没出现在词表中的词的处理方式\n",
    "    return idxs\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "#     tags=[]\n",
    "#     for idx in idxs:\n",
    "#         if idx>=len(idx2tag):\n",
    "#             tags.append('0')\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    神经网络经常批量训练数据。每一次都是基于部分序列来更新网络的权重，但是要保证一个batch内所有的序列都有同样的长度。\n",
    "    所以采用<pad>来进行了填充。\n",
    "    给rnn提供序列长度是一个很好的方法，这样可以跳过填充部分的计算．\n",
    "    \n",
    "\"\"\"\n",
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)  #推特的数量\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)   #随机打乱，permutation不直接在原来的数组上进行操作，而是返回一个新的打乱顺序的数组，并不改变原来的数组\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size   #看可以形成多少个batchs\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1     #最后一部分不足一个batch_size个数的元素也形成一个batc，只是后续需要进行padding\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples) #主要是为最后一个考虑\n",
    "        current_batch_size = batch_end - batch_start    #正常来说的等于batch_size的\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:   #遍历该batch中每一个下标\n",
    "            x_list.append(words2idxs(tokens[idx]))  #根据下标挑选出对应的词！然后在词典里面找下标（一定是这样才行）,应该是通过这个方式将下标控制在0-5000之间\n",
    "            y_list.append(tags2idxs(tags[idx]))      \n",
    "            max_len_token = max(max_len_token, len(tags[idx]))    #最大长度的标签的长度\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        #保证token和tag的长度是一样的\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "    \n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        \n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "#         print(\"序列长度：\", lengths.shape)\n",
    "        for n in range(current_batch_size):    #用来记录每一个sequence的真实长度\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network\n",
    "\n",
    "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    创建一个LSTM网络，为句子中的每个标记生成标签概率分布。要考虑token的左右侧的上下文。\n",
    "    密集层将用于标签分类 \n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we need to create [placeholders](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder) to specify what data we are going to feed into the network during the execution time.  For this task we will need the following placeholders:\n",
    " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
    " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
    "\n",
    "It could be noticed that we use *None* in the shapes in the declaration, which means that data of any size can be feeded. \n",
    "\n",
    "You need to complete the function *declare_placeholders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    初始化一些赋值变量：即创建placeholders．在程序执行期间，才会给变量赋值\n",
    "    词序列\n",
    "    标签序列\n",
    "    没有填充的序列长度：即 batch_size\n",
    "    丢弃概率\n",
    "    学习速率\n",
    "\"\"\"\n",
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    # 每一次处理batch_size个推特，每个推特的长度为sequence_len\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch')  #batch_size * sequence_len\n",
    "    print(\"self.input_batch.shape:\", self.input_batch.shape)\n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name=\"ground_truth_tags\") #batch_size * sequence_len\n",
    "  \n",
    "    # Placeholder for lengths of the sequences.(序列长度)\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') #batch_size\n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    # tf.cast　改变一个张量的数据类型（保留概率）\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let us specify the layers of the neural network. First, we need to perform some preparatory steps: \n",
    " \n",
    "- Create embeddings matrix with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable). Specify its name (*embeddings_matrix*), type  (*tf.float32*), and initialize with random values.\n",
    "- Create forward and backward LSTM cells. TensorFlow provides a number of [RNN cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods) ready for you. We suggest that you use *BasicLSTMCell*, but you can also experiment with other types, e.g. GRU cells. [This](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blogpost could be interesting if you want to learn more about the differences.\n",
    "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
    " \n",
    "After that, you can build the computation graph that transforms an input_batch:\n",
    "\n",
    "- [Look up](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) embeddings for an *input_batch* in the prepared *embedding_matrix*.\n",
    "- Pass the embeddings through [Bidirectional Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) with the specified forward and backward cells. Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
    "- Create a dense layer on top. Its output will be used directly in loss function.  \n",
    " \n",
    "Fill in the code below. In case you need to debug something, the easiest way is to check that tensor shapes of each step match the expected ones. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"#表示神经网络的层\n",
    "    创建词嵌入矩阵\n",
    "    创建前向、后向 LSTM cell，类型选择Basic LSTMCell\n",
    "    用DropoutWrapper对cell进行正则化\n",
    "    之后可以将每个输入batch转换为一个计算图\n",
    "        1.从之前准备好的embedding_matrix中为input_batch进行查找，组成embedding矩阵\n",
    "        2.通过带有特定的前向，后向cells的双向动态RNN来处理这个embeddings(可以使用长度占位符来避免RNN对填充标记长度的计算)\n",
    "        3.在顶部创建一个密集层，他的输出将会用于直接计算损失函数\n",
    "\"\"\"\n",
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    # V * N\n",
    "    # 词嵌入矩阵初始化\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype=tf.float32, name=\"embeddings_matrix\")   #这里的代码没验证正确性\n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell =  tf.contrib.rnn.BasicLSTMCell(n_hidden_rnn, forget_bias=1.0, state_is_tuple=True)   #正方向传播的RNN\n",
    "    tf.contrib.rnn.DropoutWrapper(forward_cell)\n",
    "    backward_cell =  tf.contrib.rnn.BasicLSTMCell(n_hidden_rnn, forget_bias=1.0, state_is_tuple=True)    #反方向传播的RNN\n",
    "    tf.contrib.rnn.DropoutWrapper(backward_cell)\n",
    "    \n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "    print(embeddings.shape)    #(?,?,200)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    \"\"\"yes, 原来错在sequence_length上面\"\"\"\n",
    "    # 现在报的是TypeError错误，在文档中看到一句：TypeError: If cell_fw or cell_bw is not an instance of RNNCell.\n",
    "    # 经过测试，错误在forget_bias = self.dropout_ph(self.dropout_ph这玩意现在根本就不能用，改成forget_bias就没事)\n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs=embeddings, sequence_length=self.lengths, dtype=tf.float32)\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)  #以前版本的形式\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].  形成全连接层 \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the actual predictions of the neural network, you need to apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#应用softmax函数到最后一层，用argmax来找到最可能的标签\n",
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    # tf.nn.softmax(logits, axis=None, name=None, dim=None)\n",
    "    softmax_output = tf.nn.softmax(self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    print(\"softmax_output:\", softmax_output)\n",
    "    self.predictions = tf.argmax(softmax_output, axis=-1)    #概率最大的类别的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "During training we do not need predictions of the network, but we need a loss function. We will use [cross-entropy loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), efficiently implemented in TF as \n",
    "[cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits). Note that it should be applied to logits of the model (not to softmax probabilities!). Also note,  that we do not want to take into account loss terms coming from `<PAD>` tokens. So we need to mask them out, before computing [mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用交叉熵来做损失函数，在计算损失时，不需要将<PAD>token考虑在内\n",
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels= ground_truth_tags_one_hot)\n",
    "    #返回逐个元素x!=y的布尔值 not_equal(x,y,name=None)\n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    # Be careful that the argument of tf.reduce_mean should be\n",
    "    # multiplication of mask and loss_tensor.\n",
    "    self.loss =  tf.reduce_mean(tf.matmul(mask, loss_tensor,  transpose_a=False, transpose_b=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to specify is how we want to optimize the loss. \n",
    "We suggest that you use [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) optimizer with a learning rate from the corresponding placeholder. \n",
    "You will also need to apply [clipping](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_clipping) to eliminate exploding gradients. It can be easily done with [clip_by_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#优化损失函数2\n",
    "#选择合适的优化函数和进行梯度剪切\n",
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    # 千万别加上minimize(self.loss)，不然它就不是一个优化器了阿．．．．\n",
    "    self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    # grads_and_vars: compute_gradients()函数返回的(gradient, variable)对的列表 \n",
    "    # 卧槽，原始是可以用的，只是我为啥要加上minimize()!!!,终于通过了．．．．．．．．．．\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)  \n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)   #梯度剪切\n",
    "    self.grads_and_vars =  [(tf.clip_by_norm( grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)   #system supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations! You have specified all the parts of your network. You may have noticed, that we didn't deal with any real data yet, so what you have written is just recipes on how the network should function.\n",
    "Now we will put them to the constructor of our Bi-LSTM class to use it in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    现在已经写完了我的网络的所有部分，但是还没有使用任何真实的数据，下一个模块会应用写好的Bi_LSTM\n",
    "\"\"\"\n",
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()   #声明需要后期赋值的变量\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)  #建立我的整个网络层\n",
    "    self.__compute_predictions()   #将全连接层的输出放进softmax函数，得出预测结果\n",
    "    self.__compute_loss(n_tags, PAD_index)    #利用交叉熵损失函数，真实标签的one-hot,预测的y向量来计算平均损失函数\n",
    "    self.__perform_optimization()   #利用Adam，梯度剪切来优化损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*, which was declared in *perform_optimization*. To predict tags, we just need to compute *self.predictions*. Anyway, we need to feed actual data through the placeholders that we defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    利用x_batch和y_batch进行训练\n",
    "\"\"\"\n",
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *predict_for_batch* by initializing *feed_dict* with input *x_batch* and *lengths* and running the *session* for *self.predictions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    对测试集的x_batch进行预测\n",
    "\"\"\"\n",
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    feed_dict = {\n",
    "        self.input_batch:x_batch,\n",
    "        self.lengths:lengths        \n",
    "                }\n",
    "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
    "    \n",
    "    return predictions     #这里的返回值是一个numpy.arra,千万不能写self.predictions,那是还没有计算之前的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished with necessary methods of our BiLSTMModel model and almost ready to start experimenting.\n",
    "\n",
    "### Evaluation \n",
    "To simplify the evaluation process we provide two functions for you:\n",
    " - *predict_tags*: uses a model to get predictions and transforms indices to tokens and tags;\n",
    " - *eval_conll*: calculates precision, recall and F1 for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#看传进来的token_idxs_batch是不是打乱的，如果是打乱的，下面的token,tag元组列表就有意义\n",
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    #这里的tag_idxs_batch是一个Tensor\n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths) #预测出的tag下标列表\n",
    "#     for tag_idx in tag_idxs_batch:\n",
    "#         print(\"tag_idx:\", tag_idx)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):  \n",
    "        tags, tokens = [], []\n",
    "        #这种写法可能是想少写一个循环，也就是一个循环循环了两个list\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):  #标签和token的下标元组\n",
    "            tags.append(idx2tag[tag_idx])        #下标对应的tag,这里出错，找不到下标对应的tag,因为实际tag的维度只有21,但是softmax输出的n_tag却有50维\n",
    "            tokens.append(idx2token[token_idx])  #下标对应的token\n",
    "        tags_batch.append(tags)     #真实字符串的list\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch    #预测的tag列表   和   tokens列表(真实的字符串，不是下标)\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "#         print(\"x_batch:\", x_batch)   #这里下标超出5000太多，无法进行embedding查找\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):     #一直没搞清楚token长度和tag长度有什么关系\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []     #预测标签\n",
    "        ground_truth_tags = []  #真实标签\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])   #真实标签的集合\n",
    "                predicted_tags.append(pred_tag)    #预测标签的集合\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])    #用来表示一次序列的结束\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    #求精确度，召回率，和F1值\n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create *BiLSTMModel* model with the following parameters:\n",
    " - *vocabulary_size* — number of tokens;\n",
    " - *n_tags* — number of tags;\n",
    " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
    " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
    " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
    "\n",
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- 4 epochs;\n",
    "- starting value of *learning_rate*: 0.005\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n",
    "\n",
    "However, feel free to conduct more experiments to tune hyperparameters and earn extra points for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.input_batch.shape: (?, ?)\n",
      "(?, ?, 200)\n",
      "softmax_output: Tensor(\"Reshape_1:0\", shape=(?, ?, 21), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  #清除默认图形堆栈并重置全局默认图形\n",
    "\n",
    "# model = BiLSTMModel( 5000, 50, 200, 200, 1)\n",
    "model = BiLSTMModel( 5000, 21, 200, 200, 1)   #由于tag实际类别为21的问题，修改了下\n",
    "\n",
    "batch_size = 32 ######### YOUR CODE HERE #############\n",
    "n_epochs = 4 ######### YOUR CODE HERE #############\n",
    "learning_rate = 0.005 ######### YOUR CODE HERE #############\n",
    "learning_rate_decay =  np.sqrt(2)######### YOUR CODE HERE #############\n",
    "dropout_keep_probability = 0.9 ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got an error *\"Tensor conversion requested dtype float64 for Tensor with dtype float32\"* in this point, check if there are variables without dtype initialised. Set the value of dtype equals to *tf.float32* for such variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 71725 phrases; correct: 171.\n",
      "\n",
      "precision:  0.24%; recall:  3.81%; F1:  0.45\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 8655 phrases; correct: 27.\n",
      "\n",
      "precision:  0.31%; recall:  5.03%; F1:  0.59\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 1980 phrases; correct: 1000.\n",
      "\n",
      "precision:  50.51%; recall:  22.28%; F1:  30.92\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 223 phrases; correct: 106.\n",
      "\n",
      "precision:  47.53%; recall:  19.74%; F1:  27.89\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 3224 phrases; correct: 1814.\n",
      "\n",
      "precision:  56.27%; recall:  40.41%; F1:  47.04\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 363 phrases; correct: 175.\n",
      "\n",
      "precision:  48.21%; recall:  32.59%; F1:  38.89\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 3397 phrases; correct: 2264.\n",
      "\n",
      "precision:  66.65%; recall:  50.43%; F1:  57.42\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 376 phrases; correct: 186.\n",
      "\n",
      "precision:  49.47%; recall:  34.64%; F1:  40.74\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()    #建立会话\n",
    "sess.run(tf.global_variables_initializer())   #对会话进行初始化\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "          \n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "#         print(\"x_batch:\", x_batch)\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see full quality reports for the final model on train, validation, and test sets. To give you a hint whether you have implemented everything correctly, you might expect F-score about 40% on the validation set.\n",
    "\n",
    "**The output of the cell below (as well as the output of all the other cells) should be present in the notebook for peer2peer review!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 3696 phrases; correct: 2729.\n",
      "\n",
      "precision:  73.84%; recall:  60.79%; F1:  66.68\n",
      "\n",
      "\t     company: precision:   79.37%; recall:   67.03%; F1:   72.68; predicted:   543\n",
      "\n",
      "\t    facility: precision:   72.54%; recall:   68.15%; F1:   70.28; predicted:   295\n",
      "\n",
      "\t     geo-loc: precision:   86.75%; recall:   72.99%; F1:   79.28; predicted:   838\n",
      "\n",
      "\t       movie: precision:   33.33%; recall:   30.88%; F1:   32.06; predicted:    63\n",
      "\n",
      "\t musicartist: precision:   61.50%; recall:   49.57%; F1:   54.89; predicted:   187\n",
      "\n",
      "\t       other: precision:   72.63%; recall:   54.69%; F1:   62.40; predicted:   570\n",
      "\n",
      "\t      person: precision:   71.73%; recall:   65.58%; F1:   68.51; predicted:   810\n",
      "\n",
      "\t     product: precision:   53.74%; recall:   49.69%; F1:   51.63; predicted:   294\n",
      "\n",
      "\t  sportsteam: precision:   73.56%; recall:   29.49%; F1:   42.11; predicted:    87\n",
      "\n",
      "\t      tvshow: precision:   44.44%; recall:    6.90%; F1:   11.94; predicted:     9\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 415 phrases; correct: 205.\n",
      "\n",
      "precision:  49.40%; recall:  38.18%; F1:  43.07\n",
      "\n",
      "\t     company: precision:   68.67%; recall:   54.81%; F1:   60.96; predicted:    83\n",
      "\n",
      "\t    facility: precision:   42.86%; recall:   44.12%; F1:   43.48; predicted:    35\n",
      "\n",
      "\t     geo-loc: precision:   66.27%; recall:   48.67%; F1:   56.12; predicted:    83\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     5\n",
      "\n",
      "\t musicartist: precision:   43.75%; recall:   25.00%; F1:   31.82; predicted:    16\n",
      "\n",
      "\t       other: precision:   39.29%; recall:   27.16%; F1:   32.12; predicted:    56\n",
      "\n",
      "\t      person: precision:   41.49%; recall:   34.82%; F1:   37.86; predicted:    94\n",
      "\n",
      "\t     product: precision:   17.86%; recall:   14.71%; F1:   16.13; predicted:    28\n",
      "\n",
      "\t  sportsteam: precision:   33.33%; recall:   25.00%; F1:   28.57; predicted:    15\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 449 phrases; correct: 194.\n",
      "\n",
      "precision:  43.21%; recall:  32.12%; F1:  36.85\n",
      "\n",
      "\t     company: precision:   54.39%; recall:   36.90%; F1:   43.97; predicted:    57\n",
      "\n",
      "\t    facility: precision:   41.86%; recall:   38.30%; F1:   40.00; predicted:    43\n",
      "\n",
      "\t     geo-loc: precision:   74.31%; recall:   49.09%; F1:   59.12; predicted:   109\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
      "\n",
      "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    10\n",
      "\n",
      "\t       other: precision:   25.26%; recall:   23.30%; F1:   24.24; predicted:    95\n",
      "\n",
      "\t      person: precision:   38.30%; recall:   34.62%; F1:   36.36; predicted:    94\n",
      "\n",
      "\t     product: precision:    8.70%; recall:    7.14%; F1:    7.84; predicted:    23\n",
      "\n",
      "\t  sportsteam: precision:   14.29%; recall:    6.45%; F1:    8.89; predicted:    14\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False )   #在开发集上的F1值要超过40%才行\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Could we say that our model is state of the art and the results are acceptable for the task? Definately, we can say so. Nowadays, Bi-LSTM is one of the state of the art approaches for solving NER problem and it outperforms other classical methods. Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definately small. However, the implemented model outperforms classical CRFs for this task. Even better results could be obtained by some combinations of several types of methods, e.g. see [this](https://arxiv.org/abs/1603.01354) paper if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2018.6.3\n",
    "    既然已经不是买的课了，就不把它当成一个作业了，仔细认真的搞完每个细节\n",
    "    终于完成啦，接下来优化\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
